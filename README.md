# Human vs AI Text Classification

**Author:** Muhammad Osama  
**Student ID:** 1288056  
**Course:** Natural Language Processing with Python  
**Institution:** Fanshawe College  
**LinkedIn:** [muhammad-osama-872328202](https://www.linkedin.com/in/muhammad-osama-872328202)  
**GitHub:** [MuhammadOsama380](https://github.com/MuhammadOsama380)

---

## Project Overview
This project focuses on **detecting whether a given text was written by a human or generated by AI**.  
It applies both **traditional machine learning (Random Forest)** and **deep learning (SpaCy TextCategorizer)** techniques to identify writing origins based on linguistic and semantic cues.

**Goal:** Efficiently differentiate between human-written and AI-generated text with high accuracy.

---

## Dataset
- **Name:** Human vs AI Text Classification Dataset  
- **Source:** [Kaggle Dataset](https://www.kaggle.com/datasets/aknjit/human-vs-ai-text-classification-dataset/data)  
- **Samples:** 5,000 total (2,500 per class)  
- **Columns:**  
  - `text` → message content  
  - `label` → 0 = Human, 1 = AI  
- **Balance:** Equal distribution across both classes  

---

## Exploratory Data Analysis (EDA)
- **Label Distribution:** Dataset is perfectly balanced.  
- **Word Cloud:** Shows the most frequently occurring words across both classes.  
- **Text Length Distribution:**  
  - Human texts are consistent in length.  
  - AI texts show more extreme outliers — some are significantly longer.

**Visualizations:**  
- Bar plot of label counts  
- Boxplot of text lengths  
- Word cloud representation of vocabulary  

---

## Data Preprocessing (SpaCy)
Each text sample is processed using **SpaCy (`en_core_web_lg`)** pipeline:
- Lowercasing  
- Lemmatization (root-word conversion)  
- Removal of stopwords and punctuation  
- Filtering to alphabetic tokens only  

A new column `clean_text` is created for model input.

---

## Model Development

### Model 1 – SpaCy TextCategorizer
A **neural text classification model** using SpaCy’s built-in text categorization component.

#### Training & Tuning
- Tested multiple configurations:  
  - `width`: [64, 128]  
  - `dropout`: [0.3, 0.5]  
- Each configuration trained for **5 epochs**
- Evaluated using accuracy, precision, recall, and F1-score  
- Best configuration: **width = 64, dropout = 0.5**

#### Results (SpaCy)
- **Accuracy:** 99.1%  
- **Precision:** 98.2%  
- **Recall:** 100%  
- **F1-score:** **0.9920**

SpaCy model achieved **exceptionally high recall**, meaning it rarely missed AI-generated texts.

---

### Model 2 – RandomForestClassifier
A **classical machine learning model** using bag-of-words representation via `CountVectorizer`.

#### Training & Tuning
- Hyperparameters tested:
  - `n_estimators`: [100, 200]
  - `max_depth`: [10, 20]
- Trained and evaluated with same dataset split as SpaCy model.

#### Results (Random Forest)
- **Accuracy:** 97.4%  
- **Precision:** 100%  
- **Recall:** 94.8%  
- **F1-score:** **0.9730**

RandomForest achieved perfect precision but slightly lower recall compared to SpaCy.

---

## Model Comparison

| Model | Accuracy | Precision | Recall | F1-Score |
|--------|-----------|------------|---------|-----------|
| **SpaCy TextCategorizer** | **99.1%** | 98.2% | **100%** | **0.9920** |
| **RandomForestClassifier** | 97.4% | **100%** | 94.8% | 0.9730 |

**Interpretation:**
- SpaCy’s neural model captures deeper **semantic and contextual** relationships.
- RandomForest is faster and more interpretable but depends on word frequency features.
- SpaCy generalizes better for complex sentence structures, while RandomForest is computationally efficient for simpler text.

---

## Visualization Results
- **Confusion Matrix:** Displayed high classification accuracy for both models.  
- **Bar Chart:** Compared Accuracy, Precision, Recall, and F1-Score across SpaCy and RandomForest models.

---

## Insights & Discussion
- SpaCy achieved superior recall, meaning it can identify almost all AI-generated texts correctly.  
- RandomForest performed well but missed some AI samples (lower recall).  
- Word embedding–based models (like SpaCy) capture subtle linguistic cues distinguishing AI text from human writing.  
- Computational cost:  
  - SpaCy is heavier during training but offers better semantic understanding.  
  - RandomForest is lightweight and faster to deploy.

---

## Final Conclusion
Both models performed exceptionally well, achieving near-perfect accuracy.  
However, **SpaCy TextCategorizer (F1 = 0.9920)** slightly outperformed **RandomForest (F1 = 0.9730)** due to its ability to learn contextual relationships between words.  
For small to medium datasets, RandomForest remains a practical and interpretable option.  
For deployment requiring semantic depth and robustness, SpaCy is the preferred choice.

---

## How to Run
```bash
# Clone repository
git clone https://github.com/MuhammadOsama380/Human-vs-AI-Text-Classification.git
cd Human-vs-AI-Text-Classification

# Install dependencies
pip install -r requirements.txt

# Run notebook
jupyter notebook notebooks/NLP\ Project\ 2_Muhammad\ Osama.ipynb
```

---

## Tools & Libraries
- Python 3.x  
- Pandas  
- NumPy  
- Matplotlib / Seaborn  
- SpaCy  
- Scikit-learn  
- WordCloud  

---

## Repository Structure
```
Human-vs-AI-Text-Classification/
│
├── data/
│   └── your_dataset_5000.csv
│
├── notebooks/
│   └── NLP Project 2_Muhammad Osama.ipynb
│
├── presentation/
│   └── Human_vs_AI_Text_Classification_Muhammad Osama.pptx
│
├── README.md
├── requirements.txt
├── LICENSE
└── .gitignore
```

---

## License
This project is licensed under the **MIT License** — see the [LICENSE](LICENSE) file for details.

---

## Acknowledgements
Developed by **Muhammad Osama (1288056)** as part of the **Natural Language Processing** coursework at **Fanshawe College**.  
If you found this project helpful, please ⭐ the repository or connect on [LinkedIn](https://www.linkedin.com/in/muhammad-osama-872328202).
